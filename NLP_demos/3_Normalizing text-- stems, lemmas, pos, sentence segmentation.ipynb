{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing text\n",
    "\n",
    "Normalizing text means to put all the text into some standard form. There are various ways to normalize text and which you choose to use will vary depending on your application. Here are some common normalization techniques:\n",
    "* lower case\n",
    "* remove punctuation and/or numbers\n",
    "* stem \n",
    "* lemmatize\n",
    "\n",
    "The following code cell demonstrates several techniques. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence tokenization and word tokenization\n",
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "\n",
    "sentences = \"\"\"The quick brown fox jumped over the lazy river. She sells\n",
    "sea shells by the sea shore. Humpy Dumpy sat on the wall.\"\"\"\n",
    "\n",
    "sents = nltk.sent_tokenize(sentences)\n",
    "tokens = nltk.word_tokenize(sents[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The quick brown fox jumped over the lazy river.\n",
      "['The', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'river', '.']\n"
     ]
    }
   ],
   "source": [
    "print(sents[0])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the quick brown fox jumped over the lazy river  she sells sea shells by the sea shore  humpy dumpy sat on the wall \n"
     ]
    }
   ],
   "source": [
    "# if you just want the words you can remove everything else with regex\n",
    "# remove punctuation symbols, newlines, and digits\n",
    "\n",
    "import re\n",
    "\n",
    "text = re.sub(r'[.?!,:;()\\-\\n\\d]',' ', sentences.lower())\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stemming\n",
    "\n",
    "Stemming removes affixes from words. A well-known stemmer is the Porter stemmer, available in NLTK.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'quick', 'brown', 'fox', 'jump', 'over', 'the', 'lazi', 'river', '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "porter = nltk.PorterStemmer()\n",
    "stemmed = [porter.stem(t) for t in tokens]\n",
    "stemmed[:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lemmatizing\n",
    "\n",
    "Lemmatizing attempts to take a word down to its base lexical form, as you would find in a dictionary. NLTK can do that as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'river', '.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnl = nltk.WordNetLemmatizer()\n",
    "lemmatized = [wnl.lemmatize(t) for t in tokens]\n",
    "lemmatized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Notice that 'jumped' did not get lemmatize. The lemmatizer works better if it knows whether a word is a verb.\n",
    "\n",
    "\n",
    "### pos\n",
    "\n",
    "NLTK has a pos tagger that inputs a list of tokens and outputs a list of tuples in the form (token, pos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'),\n",
       " ('quick', 'JJ'),\n",
       " ('brown', 'NN'),\n",
       " ('fox', 'NN'),\n",
       " ('jumped', 'VBD'),\n",
       " ('over', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('lazy', 'JJ'),\n",
       " ('river', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = nltk.pos_tag(tokens)\n",
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'quick', 'brown', 'fox', 'jump', 'over', 'the', 'lazy', 'river', '.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatized = []\n",
    "for token, tag in tags:\n",
    "    if tag.startswith('VB'):\n",
    "        lemma = wnl.lemmatize(token, pos='v')\n",
    "    else:\n",
    "        lemma = wnl.lemmatize(token)\n",
    "    lemmatized.append(lemma)\n",
    "lemmatized\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### order matters\n",
    "\n",
    "Order matters with text processing. For example, if you lower case and remove punctuation, it will be very hard for nltk to segment text into sentences. So often it's a good idea to keep the original text in a variable for things like sentence segmentation and make text processing changes to a copy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
