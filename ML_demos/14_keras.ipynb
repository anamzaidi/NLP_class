{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Example\n",
    "\n",
    "Using an SMS Spam data set (slightly modified) from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/sms+spam+collection). The data set is a collection of 5574 SMS messages that have been labeled as ham or spam. The file is a tab-delimited file with the first column the label and the second the message content. I edited the data set to remove some unwanted columns and add headings. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# some necessary packages\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# set seed for reproducibility\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows and columns: (4837, 2)\n",
      "   spam                                               text\n",
      "0     0  Go until jurong point, crazy.. Available only ...\n",
      "1     0                      Ok lar... Joking wif u oni...\n",
      "2     1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3     0  U dun say so early hor... U c already then say...\n",
      "4     0  Nah I don't think he goes to usf, he lives aro...\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('sms-spam.csv', header=0, usecols=[1,2], encoding='latin-1')\n",
    "print('rows and columns:', df.shape)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data size:  (3863, 2)\n",
      "test data size:  (974, 2)\n"
     ]
    }
   ],
   "source": [
    "# split df into train and test\n",
    "i = np.random.rand(len(df)) < 0.8\n",
    "train = df[i]\n",
    "test = df[~i]\n",
    "print(\"train data size: \", train.shape)\n",
    "print(\"test data size: \", test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shapes: (3863, 25000) (3863,)\n",
      "test shapes: (974, 25000) (974,)\n",
      "test first five labels: [0 0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "# set up X and Y\n",
    "num_labels = 2\n",
    "vocab_size = 25000\n",
    "batch_size = 100\n",
    "\n",
    "# fit the tokenizer on the training data\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(train.text)\n",
    "\n",
    "x_train = tokenizer.texts_to_matrix(train.text, mode='tfidf')\n",
    "x_test = tokenizer.texts_to_matrix(test.text, mode='tfidf')\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(train.spam)\n",
    "y_train = encoder.transform(train.spam)\n",
    "y_test = encoder.transform(test.spam)\n",
    "\n",
    "# check shape\n",
    "print(\"train shapes:\", x_train.shape, y_train.shape)\n",
    "print(\"test shapes:\", x_test.shape, y_test.shape)\n",
    "print(\"test first five labels:\", y_test[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_18 (Dense)             (None, 32)                800032    \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 800,065\n",
      "Trainable params: 800,065\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 3476 samples, validate on 387 samples\n",
      "Epoch 1/30\n",
      "3476/3476 [==============================] - 3s - loss: 0.5894 - acc: 0.7621 - val_loss: 0.4307 - val_acc: 0.9354\n",
      "Epoch 2/30\n",
      "3476/3476 [==============================] - 1s - loss: 0.3137 - acc: 0.9497 - val_loss: 0.2250 - val_acc: 0.9716\n",
      "Epoch 3/30\n",
      "3476/3476 [==============================] - 1s - loss: 0.1523 - acc: 0.9807 - val_loss: 0.1265 - val_acc: 0.9845\n",
      "Epoch 4/30\n",
      "3476/3476 [==============================] - 1s - loss: 0.0789 - acc: 0.9942 - val_loss: 0.0861 - val_acc: 0.9897\n",
      "Epoch 5/30\n",
      "3476/3476 [==============================] - 1s - loss: 0.0469 - acc: 0.9965 - val_loss: 0.0676 - val_acc: 0.9922\n",
      "Epoch 6/30\n",
      "3476/3476 [==============================] - 1s - loss: 0.0307 - acc: 0.9977 - val_loss: 0.0574 - val_acc: 0.9922\n",
      "Epoch 7/30\n",
      "3476/3476 [==============================] - 1s - loss: 0.0216 - acc: 0.9988 - val_loss: 0.0515 - val_acc: 0.9922\n",
      "Epoch 8/30\n",
      "3476/3476 [==============================] - 1s - loss: 0.0160 - acc: 0.9991 - val_loss: 0.0481 - val_acc: 0.9922\n",
      "Epoch 9/30\n",
      "3476/3476 [==============================] - 1s - loss: 0.0124 - acc: 0.9994 - val_loss: 0.0455 - val_acc: 0.9922\n",
      "Epoch 10/30\n",
      "3476/3476 [==============================] - 1s - loss: 0.0097 - acc: 0.9994 - val_loss: 0.0438 - val_acc: 0.9922\n",
      "Epoch 11/30\n",
      "3476/3476 [==============================] - 1s - loss: 0.0076 - acc: 0.9994 - val_loss: 0.0429 - val_acc: 0.9922\n",
      "Epoch 12/30\n",
      "3476/3476 [==============================] - 1s - loss: 0.0062 - acc: 0.9997 - val_loss: 0.0422 - val_acc: 0.9922\n",
      "Epoch 13/30\n",
      "3476/3476 [==============================] - 1s - loss: 0.0052 - acc: 1.0000 - val_loss: 0.0418 - val_acc: 0.9922\n",
      "Epoch 14/30\n",
      "3476/3476 [==============================] - 1s - loss: 0.0043 - acc: 1.0000 - val_loss: 0.0415 - val_acc: 0.9922\n",
      "Epoch 15/30\n",
      "3476/3476 [==============================] - 1s - loss: 0.0037 - acc: 1.0000 - val_loss: 0.0415 - val_acc: 0.9922\n",
      "Epoch 16/30\n",
      "3476/3476 [==============================] - 1s - loss: 0.0032 - acc: 1.0000 - val_loss: 0.0414 - val_acc: 0.9922\n",
      "Epoch 17/30\n",
      "3476/3476 [==============================] - 1s - loss: 0.0028 - acc: 1.0000 - val_loss: 0.0413 - val_acc: 0.9922\n",
      "Epoch 18/30\n",
      "3476/3476 [==============================] - 1s - loss: 0.0025 - acc: 1.0000 - val_loss: 0.0414 - val_acc: 0.9922\n",
      "Epoch 19/30\n",
      "3476/3476 [==============================] - 1s - loss: 0.0022 - acc: 1.0000 - val_loss: 0.0416 - val_acc: 0.9922\n",
      "Epoch 20/30\n",
      "3476/3476 [==============================] - 1s - loss: 0.0020 - acc: 1.0000 - val_loss: 0.0416 - val_acc: 0.9922\n",
      "Epoch 21/30\n",
      "3476/3476 [==============================] - 1s - loss: 0.0018 - acc: 1.0000 - val_loss: 0.0418 - val_acc: 0.9922\n",
      "Epoch 22/30\n",
      "3476/3476 [==============================] - 1s - loss: 0.0016 - acc: 1.0000 - val_loss: 0.0419 - val_acc: 0.9922\n",
      "Epoch 23/30\n",
      "3476/3476 [==============================] - 1s - loss: 0.0015 - acc: 1.0000 - val_loss: 0.0419 - val_acc: 0.9922\n",
      "Epoch 24/30\n",
      "3476/3476 [==============================] - 1s - loss: 0.0013 - acc: 1.0000 - val_loss: 0.0422 - val_acc: 0.9922\n",
      "Epoch 25/30\n",
      "3476/3476 [==============================] - 1s - loss: 0.0012 - acc: 1.0000 - val_loss: 0.0425 - val_acc: 0.9922\n",
      "Epoch 26/30\n",
      "3476/3476 [==============================] - 1s - loss: 0.0011 - acc: 1.0000 - val_loss: 0.0426 - val_acc: 0.9922\n",
      "Epoch 27/30\n",
      "3476/3476 [==============================] - 1s - loss: 0.0010 - acc: 1.0000 - val_loss: 0.0428 - val_acc: 0.9922\n",
      "Epoch 28/30\n",
      "3476/3476 [==============================] - 1s - loss: 9.5882e-04 - acc: 1.0000 - val_loss: 0.0432 - val_acc: 0.9922\n",
      "Epoch 29/30\n",
      "3476/3476 [==============================] - 1s - loss: 8.8899e-04 - acc: 1.0000 - val_loss: 0.0433 - val_acc: 0.9897\n",
      "Epoch 30/30\n",
      "3476/3476 [==============================] - 1s - loss: 8.3161e-04 - acc: 1.0000 - val_loss: 0.0436 - val_acc: 0.9897\n"
     ]
    }
   ],
   "source": [
    "# fit Keras model\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=vocab_size, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    " \n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    " \n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=30,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900/974 [==========================>...] - ETA: 0sAccuracy:  0.980492812285\n"
     ]
    }
   ],
   "source": [
    "# evaluate\n",
    "score = model.evaluate(x_test, y_test, batch_size=batch_size, verbose=1)\n",
    "print('Accuracy: ', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.10145307583944753, 0.98049281228494345]\n"
     ]
    }
   ],
   "source": [
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "928/974 [===========================>..] - ETA: 0s[[0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "# get predictions so we can calculate more metrics\n",
    "pred = model.predict_classes(x_test)\n",
    "print(pred[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score:  0.980492813142\n",
      "precision score:  1.0\n",
      "recall score:  0.848\n",
      "f1 score:  0.917748917749\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "print('accuracy score: ', accuracy_score(y_test, pred))\n",
    "print('precision score: ', precision_score(y_test, pred))\n",
    "print('recall score: ', recall_score(y_test, pred))\n",
    "print('f1 score: ', f1_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was by far the most accurate. Much more work could be done modifying the network topology. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
